# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.

import scrapy
import operator
import csv

from scrapy.xlib.pydispatch import dispatcher

def generate_urls(country, pages):
	base_url = "https://www.airfrov.com/requests?sourceCountry=" + country + "&category=All%20Categories&query=&sortBy=Latest&page="
	urls = []
	for i in range(1, pages + 1):
		urls.append(base_url + str(i))

	print urls
	return urls

class TestSpider(scrapy.Spider):
    name = "test"
    allowed_domains = ["airfrov.com"]
    #Need autogenerated URL for each page
    start_urls = generate_urls('Japan', 38)

    def __init__(self):
    	self.requests = []
    	dispatcher.connect(self.spider_closed, scrapy.signals.spider_closed)

    def parse(self, response):
    	for card in response.xpath('//div[@class="card"]'):
    		link = card.xpath('.//a[@class="divlink"]//@href').extract()
    		name = card.xpath('.//div[@class="base-image"]//p/text()').extract()[0]
    		price = card.xpath('.//p[@class="sub"]/text()').extract()

    		state = card.xpath('.//div[@class="base-image"]//span/text()').extract()

    		#Split after the D of SGD
    		price_processed = float(price[-1].split('D ')[-1])

    		"""
    		print link
    		print name
    		print price_processed
    		print state_cleaned

    		"""

    		state_cleaned = str(state[0])

    		if state_cleaned == "OPEN" or str(state[0]) == "OFFERED":
	    		self.requests.append({'name' : str(name), 'price' : price_processed, 
	    			'link' : '=LIEN_HYPERTEXTE("https://www.airfrov.com' + str(link[0]) + '")', 'state' : state_cleaned})

    def spider_closed(self):
    	requests = sorted(self.requests, key = operator.itemgetter('price'))

    	print "------------"
    	print "Number of requests: " + str(len(requests))
    	for request in requests:
    		#print request
    		pass

    	with open('mycsv.csv', 'wb') as f:
    		w = csv.DictWriter(f, requests[0].keys())
    		w.writeheader()

    		for row in requests:
    			w.writerow(row)


