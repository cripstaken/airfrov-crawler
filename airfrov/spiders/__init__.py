# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.

import scrapy
import operator
import csv
from airfrov.items import AirfrovItem

from scrapy.xlib.pydispatch import dispatcher

def generate_urls(country, pages):
	base_url = "https://www.airfrov.com/requests?sourceCountry=" + country + "&category=All%20Categories&query=&sortBy=Latest&page="
	urls = []
	for i in range(1, pages + 1):
		urls.append(base_url + str(i))

	print urls
	return urls

class TestSpider(scrapy.Spider):
    name = "test"
    allowed_domains = ["airfrov.com"]
    #Need autogenerated URL for each page
    start_urls = generate_urls('France', 1)

    def __init__(self):
    	self.requests = []

    def parse(self, response):
    	for card in response.xpath('//div[@class="card"]'):
    		request = AirfrovItem()

    		link = card.xpath('.//a[@class="divlink"]//@href').extract()
    		name = card.xpath('.//div[@class="base-image"]//p/text()').extract()[0]
    		price = card.xpath('.//p[@class="sub"]/text()').extract()

    		state = card.xpath('.//div[@class="base-image"]//span/text()').extract()

    		#Split after the D of SGD
    		price_processed = float(price[-1].split('D ')[-1])
    		state_cleaned = str(state[0])
    		full_link = '=LIEN_HYPERTEXTE("https://www.airfrov.com' + str(link[0]) + '")'

    		request['name'] = name
    		request['price'] = price_processed
    		request['state'] = state_cleaned
    		request['link'] = full_link

    		yield request


